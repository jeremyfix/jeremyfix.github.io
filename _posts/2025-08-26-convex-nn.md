---
layout: post
title: Convex Neural Networks
date: 2025-08-28 11:00:00
description: How to look for convex neural networks
tags: convex neural networks
categories: optimization
featured: true
related_publications: true
---

In a talk by A. Gagneux at Gretsi, the topic was about looking for convex neural
networks. 

There are several motivations for looking for this subset of neural networks.
One of them comes from the plug-and-play optimizers where a denoiser is used to
regularize an optimization in place of either an explicit regularization or a
proximal operator. As far as I understand, when using a denoiser for implementing the regularization, a
sufficient condition on the denoiser is that it is the gradient of a convex
function. Hence, if you find a class of neural networks that are convex, taking
its gradient gives you a class of appropriate denoisers.

The talk began with the `Input Convex Neural Networls` {% cite Amos2017 %}. The
ICNNs is a general class of convex multi-layer neural networks. In the paper,
the authors define this class as the multilayer feedforward neural networks with
non-negative weights and convex non-decreasing activation functions. Examples
are ReLu networks with positive weights. To implement this, you just need to
clip the negative components of the weights during training.

In {% cite Gagneux2025 %}, it is shown that ICNN does not give the whole picture
of convex neural networks. Some convex neural networks do not belong to the ICNN
class. And actually, one hope is to increase the expressivness of the identified
family of convex neural networks, hence the need to go beyond ICNN. In {% cite
Gagneux2025 %}, the constraint on the positivity of the weights is relaxed and
it is demonstrated that Relux neural networks can be convex if some products of
weights are positive which is less constrainted than requiring all the weights
to be positive.

The work of {% cite Gagneux2025 %} also includes a differentiable way to check
a ReLU network is convex and therefore an ability to inject that as a
regularization term in the training loss. 

