---
layout: post
title: Regularized gradient descent
date: 2025-08-26 10:00:00
description: Plug-and-play algorithms
tags: plug-and-play proximal
categories: optimization
featured: true
related_publications: true
---

I have been attending a talk discussing inverse problems optimization using
plug-and-play algorithms. During the talk, the speaker made a quick and clear
path from regularized gradient descent to plug-and-play approaches.

Here it is. When you want to constrain your optimization problem to find
solutions, you can classically add a penalty term, or so-called regularization
term to the loss function.

For example, consider a regularized least square with a radial basis function
representation of your input. In the expression below, we denote by $y$ the
vector of all the outputs to be predicted, $w$ the amplitudes of the gaussians
of the RBF. We do not specify it further but, to fix the ideas, $\Phi$ would be
the function whose output components contain the evaluation of the individual
RBF functions evaluated on the data points $x$.

$$
min_w \|y - w^T \Phi(x)\|^2 + \lambda \|w\|_1
$$

This is of the general form $$f(x) + g(x)$$ where $$g$$ contains our regularization
term. Here, the regularization is the L1 norm which promotes sparsity. With classical gradient descent, you compute the gradient of this compound cost
and iterate with whichever algorithm you like (stochastic gradient descent, adam, etc...).

A step further, you could iterate one step of minimizing the likelihood
(minimizing the MSE can be seen as maximizing the likelihood of the data under
the hypothesis of a normal distribution whose mean is given by our linear
model) and one step of minimizing the constraint. Proximal gradient descent is a
step further as it allows to consider $$g(x)$$ that are not differentiable.

Proximal gradient descent {% cite Combettes2010 Parikh2014 %} iteratively updates the estimate of the solution by :

$$x_{k+1} = \text{prox}_\alpha(x_k - \alpha \nabla f(x_k))$$

with

$$\text{prox}_\alpha(z) = \text{argmin}_y (\frac{1}{2\alpha}\|y - z\|^2 + g(y))$$

The operator $$\text{prox}$$ is called the proximity operator.

The proximal operator requires a minimization. In the plug-and-play {% cite Venkatakrishnan2013 %} approach,
you replace that minimization by a denoising step. This denoising step can be
implemented with a denoising neural network.
